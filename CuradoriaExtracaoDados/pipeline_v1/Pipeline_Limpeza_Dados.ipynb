{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bibliotecas"
      ],
      "metadata": {
        "id": "kYPqZoK1VFw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código utiliza bibliotecas/ferramentas, como nltk e demoji para a limpeza e normalização dos dados:\n",
        "\n",
        "**Documentações:**\n",
        "\n",
        "- **nltk:** https://www.nltk.org/howto/portuguese_en.html\n",
        "- **demoji**: https://pypi.org/project/demoji/\n",
        "- **Enelvo**: https://github.com/thalesbertaglia/enelvo\n",
        "- **re:** https://docs.python.org/3/library/re.html\n",
        "\n",
        "Para criptografia do arquivo com usuários:\n",
        "- **cryptography:** https://pypi.org/project/cryptography/"
      ],
      "metadata": {
        "id": "4MaOY9AkVrtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "stopwords.append(\"pra\")\n",
        "stopwords.append(\"tá\")\n",
        "\n",
        "import re\n",
        "\n",
        "!pip install demoji\n",
        "import demoji\n",
        "\n",
        "!pip install enelvo\n",
        "from enelvo import normaliser\n",
        "\n",
        "from cryptography.fernet import Fernet\n",
        "import os"
      ],
      "metadata": {
        "id": "8P2_PmCuUHmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline"
      ],
      "metadata": {
        "id": "URX2SXJTW9Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para criptografar o arquivo de usuários usamos a biblioteca cryptography e criamos algumas funções:"
      ],
      "metadata": {
        "id": "rwYq3mJHYMfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gera uma chave usando o módulo Fernet e salva em um arquivo chamado \"mykey.key\"\n",
        "def generate_key():\n",
        "    key = Fernet.generate_key()\n",
        "    with open(\"mykey.key\", \"wb\") as key_file:\n",
        "        key_file.write(key)\n",
        "\n",
        "# Carrega a chave gerada anteriormente do arquivo \"mykey.key\"\n",
        "def load_key():\n",
        "    return open(\"mykey.key\", \"rb\").read()\n",
        "\n",
        "# Encripta o conteúdo do arquivo especificado usando a chave fornecida\n",
        "def encrypt(filename, key):\n",
        "    f = Fernet(key)\n",
        "    with open(filename, \"rb\") as file:\n",
        "        file_data = file.read()\n",
        "        encrypted_data = f.encrypt(file_data)\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(encrypted_data)\n",
        "\n",
        "# Decripta o conteúdo do arquivo especificado usando a chave fornecida\n",
        "def decrypt(filename, key):\n",
        "    f = Fernet(key)\n",
        "    with open(filename, \"rb\") as file:\n",
        "        encrypted_data = file.read()\n",
        "        try:\n",
        "            decrypted_data = f.decrypt(encrypted_data)\n",
        "        except:\n",
        "            print(\"Invalid key\")\n",
        "            return\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(decrypted_data)"
      ],
      "metadata": {
        "id": "p4oGftszYXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os dados foram anonimizados para proteger a privacidade dos usuários substituindo usuários (@...) por chaves no arquivo de dicionário criptografado (user_dict).\n",
        "\n",
        "Além disso, tokenizamos as palavras (sem separar datas, hashtags e números com vírgula), realizamos a conversão de caracteres maiúsculos para minúsculos, substituímos emojis (Descrição ou emojipositivo/emojinegativo), removemos URLs, pontuações e palavras irrelevantes (stop words) e\n",
        "\n",
        "Utilizamos a ferramenta enelvo para corrigir erros ortográficos.\n"
      ],
      "metadata": {
        "id": "p6XFs-2UW8JK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "RWj9JhXJ8oP2"
      },
      "outputs": [],
      "source": [
        "emoji_list = {':))': 'emojipositivo', ':)': 'emojipositivo', ':d': 'emojipositivo', ':p': 'emojipositivo',\n",
        "              ':(': 'emojinegativo', ':((': 'emojinegativo', '8)': 'emojineutro'}\n",
        "normalizador = normaliser.Normaliser(tokenizer='readable')\n",
        "\n",
        "# Função para substituir usuários em um texto usando um dicionário salvo em 'user_dict.txt'\n",
        "def substitui_user(match):\n",
        "  loaded_key= load_key()\n",
        "  user = match.group(0)\n",
        "\n",
        "  if os.stat('user_dict.txt').st_size == 0:\n",
        "    with open('user_dict.txt', 'r+') as dic:\n",
        "      lines = dic.readlines()\n",
        "      nuser = f'{user}:user{len(lines) + 1}\\n'\n",
        "      dic.write(nuser)\n",
        "      dic.close()\n",
        "\n",
        "      encrypt('user_dict.txt', loaded_key)\n",
        "      return f'user{len(lines) + 1}'\n",
        "\n",
        "  else:\n",
        "    decrypt('user_dict.txt', loaded_key)\n",
        "    with open('user_dict.txt', 'r+') as dic:\n",
        "      lines = dic.readlines()\n",
        "      for line in lines:\n",
        "        key, value = line.strip().split(\":\")\n",
        "        if key == user:\n",
        "          dic.close()\n",
        "          encrypt('user_dict.txt', loaded_key)\n",
        "          return value\n",
        "\n",
        "      nuser = f'{user}:user{len(lines) + 1}\\n'\n",
        "      dic.write(nuser)\n",
        "\n",
        "      dic.close()\n",
        "      encrypt('user_dict.txt', loaded_key)\n",
        "      return f'user{len(lines) + 1}'\n",
        "\n",
        "# Função para substituir emojis\n",
        "def substitui_emoji(text):\n",
        "    for emoji, label in emoji_list.items():\n",
        "        text = text.replace(emoji, label)\n",
        "    dem = demoji.findall(text)\n",
        "    for item, value in dem.items():\n",
        "        text = text.replace(item, f\" {value.replace(' ', '')}\")\n",
        "    return text\n",
        "\n",
        "def preprocess(texto, remove_stopwords=False, tokenization=False):\n",
        "    # Substituindo , por chavev\n",
        "    texto = texto.str.replace(r',', 'chavevirg')\n",
        "\n",
        "    # Normalizando texto\n",
        "    texto = texto.apply(lambda x: normalizador.normalise(x))\n",
        "\n",
        "    # Substituindo emojis\n",
        "    texto = texto.apply(substitui_emoji)\n",
        "\n",
        "    # Substituindo users\n",
        "    texto = texto.str.replace(r'@\\w+', substitui_user)\n",
        "\n",
        "    # Removendo URLs\n",
        "    texto = texto.str.replace(r'https?://\\S+', '')\n",
        "\n",
        "    # Substituindo chavev por ,\n",
        "    texto = texto.str.replace(r'chavevirg', ',')\n",
        "\n",
        "    # Removendo vírgulas não associadas a números e outras pontuações\n",
        "    texto = texto.apply(lambda x: re.sub(r'(?<!\\d),(?=\\D)|(?<=\\D),(?!\\d)|(?<!\\d),(?=\\d)|(?<!\\d)\\/|\\/(?!\\d)|_|[^\\w#\\/\\s,]',\n",
        "                                         '', x))\n",
        "\n",
        "    # Removendo Stop Words (opcional)\n",
        "    if remove_stopwords:\n",
        "        texto = texto.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
        "\n",
        "    # Tokenization (opcional)\n",
        "    if tokenization:\n",
        "        pattern = r\"\\b(?:\\d{1,2}/\\d{1,2}(?:/\\d{4})?)\\b|(?:\\d+,\\d+)|\\b\\w+\\b|#\\w+\\b\"\n",
        "        texto = texto.apply(lambda x: regexp_tokenize(x, pattern))\n",
        "\n",
        "    return texto"
      ]
    }
  ]
}
